{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4556b907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../data/augmented_and_segmented_various.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../../../data/augmented_and_segmented_various.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1417f0c1-9689-43f4-bfd3-354bc34f4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from crf_segmenter import MorphologicalSegmentation\n",
    "\n",
    "def evaluate_crf(model_paths):\n",
    "    \"\"\"\n",
    "    Compare multiple models based on their saved prediction files.\n",
    "    \n",
    "    Args:\n",
    "        model_paths (list): List of paths to model directories containing predictions\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing evaluation results for all models\n",
    "    \"\"\"\n",
    "    results = []  # List to store results\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating model at: {model_path}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model_name = os.path.basename(model_path)\n",
    "        predictions_dir = os.path.join(model_path, 'predictions')\n",
    "        \n",
    "        # Initialize the evaluator\n",
    "        try:\n",
    "            evaluator = Evaluator(None)  # Pass None since we don't need feature extractor for file evaluation\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Evaluator: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        # Get predictions from file\n",
    "        try:\n",
    "            results_dict, predicted, target = evaluator.get_predictions_from_file(\n",
    "                predictions_dir=predictions_dir\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading predictions: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "        # Collect results for comparison\n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'position_precision': results_dict['position']['precision'],\n",
    "            'position_recall': results_dict['position']['recall'],\n",
    "            'position_f1': results_dict['position']['f1'],\n",
    "            'bleu_scores': results_dict['bleu_scores']['equal'],\n",
    "            'chrf': results_dict['chrf'],\n",
    "            'num_predictions': len(predicted),\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print individual model results\n",
    "        print(f\"\\nResults for model: {result['model_name']}\")\n",
    "        print(f\"Number of predictions: {result['num_predictions']}\")\n",
    "        print(f\"Position Scores: Precision={result['position_precision']:.3f}, \"\n",
    "              f\"Recall={result['position_recall']:.3f}, F1={result['position_f1']:.3f}\")\n",
    "        print(f\"BLEU Score={result['bleu_scores']:.4f}\")\n",
    "        print(f\"chrF Score: {result['chrf']:.4f}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to compare - all models failed evaluation\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    comparison_file = \"model_comparison_results_file.csv\"\n",
    "    \n",
    "    if os.path.exists(comparison_file):\n",
    "        df.to_csv(comparison_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(comparison_file, index=False)\n",
    "    \n",
    "    print(f\"\\nComparison results saved to: {comparison_file}\")\n",
    "    \n",
    "    # Create a formatted markdown table\n",
    "    markdown_table = \"# Model Comparison Results\\n\\n\"\n",
    "    markdown_table += \"## Summary Statistics\\n\\n\"\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_df = df[['position_f1', 'bleu_scores', 'chrf']].agg(['mean', 'std', 'min', 'max'])\n",
    "    markdown_table += summary_df.to_markdown() + \"\\n\\n\"\n",
    "    \n",
    "    # Add full results table\n",
    "    markdown_table += \"## Detailed Results\\n\\n\"\n",
    "    markdown_table += df.to_markdown(index=False)\n",
    "    \n",
    "    with open(\"model_comparison_results_file.md\", \"w\") as f:\n",
    "        f.write(markdown_table)\n",
    "    \n",
    "    print(f\"Comparison markdown saved to: model_comparison_results.md\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a434e09-9be8-4c99-b67d-656e85b8486a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluating model at: models_finals/segmenter_one\n",
      "==================================================\n",
      "Reading predictions from: models_finals/segmenter_one/predictions/predictions_20250108_185936.csv\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "\n",
      "Successfully loaded 20000 predictions\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Example 1:\n",
      "Target: s-a-gcwalisek-a\n",
      "Predicted: s-a-gcwalisek-a\n",
      "\n",
      "Example 2:\n",
      "Target: aba-s-e-msebenz-ini\n",
      "Predicted: a-ba-s-e-msebenz-ini\n",
      "\n",
      "Example 3:\n",
      "Target: aba-n-e-nhlanhla\n",
      "Predicted: aba-n-e-n-hlanhla\n",
      "\n",
      "Results for model: segmenter_one\n",
      "Number of predictions: 20000\n",
      "Position Scores: Precision=0.850, Recall=0.855, F1=0.853\n",
      "BLEU Score=0.8736\n",
      "chrF Score: 0.9356\n",
      "\n",
      "==================================================\n",
      "Evaluating model at: models_finals/segmenter_two\n",
      "==================================================\n",
      "Reading predictions from: models_finals/segmenter_two/predictions/predictions_20250108_185952.csv\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "\n",
      "Successfully loaded 20000 predictions\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Example 1:\n",
      "Target: s-a-gcwalisek-a\n",
      "Predicted: s-a-gcwalisek-a\n",
      "\n",
      "Example 2:\n",
      "Target: aba-s-emsebenz-ini\n",
      "Predicted: a-ba-s-emsebenz-ini\n",
      "\n",
      "Example 3:\n",
      "Target: aba-ne-nhlanhla\n",
      "Predicted: aba-ne-n-hlanhla\n",
      "\n",
      "Results for model: segmenter_two\n",
      "Number of predictions: 20000\n",
      "Position Scores: Precision=0.837, Recall=0.842, F1=0.840\n",
      "BLEU Score=0.8599\n",
      "chrF Score: 0.9295\n",
      "\n",
      "==================================================\n",
      "Evaluating model at: models_finals/segmenter_three\n",
      "==================================================\n",
      "Reading predictions from: models_finals/segmenter_three/predictions/predictions_20250108_190006.csv\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "Original pairs: 20000\n",
      "Valid pairs after filtering: 20000\n",
      "Filtered out 0 pairs\n",
      "\n",
      "Successfully loaded 20000 predictions\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Example 1:\n",
      "Target: sa-gcwalisek-a\n",
      "Predicted: sa-gcwalisek-a\n",
      "\n",
      "Example 2:\n",
      "Target: abas-emsebenzini\n",
      "Predicted: aba-s-emsebenzini\n",
      "\n",
      "Example 3:\n",
      "Target: abane-nhlanhla\n",
      "Predicted: abane-nhlanhla\n",
      "\n",
      "Results for model: segmenter_three\n",
      "Number of predictions: 20000\n",
      "Position Scores: Precision=0.780, Recall=0.788, F1=0.784\n",
      "BLEU Score=0.8033\n",
      "chrF Score: 0.9108\n",
      "\n",
      "Comparison results saved to: model_comparison_results_file.csv\n",
      "Comparison markdown saved to: model_comparison_results.md\n"
     ]
    }
   ],
   "source": [
    "from evaluator import Evaluator  # Make sure this import matches your file structure\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_paths = [\n",
    "        \"models_finals/segmenter_one\",\n",
    "        \"models_finals/segmenter_two\",\n",
    "        \"models_finals/segmenter_three\"\n",
    "    ]\n",
    "    results_df = evaluate_crf(model_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
