{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4762b1-ee5a-44cb-ae4f-c80c361fbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn_crfsuite, nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44548737-1058-468b-866f-d398edd28c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import sklearn_crfsuite\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.chrf_score import corpus_chrf\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2e98329-f3da-4655-9ebf-1cc09d6d0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: /workspace/segmentation-models/crf_model/models/segmenter_three\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"segmenter_one\"\n",
    "pwd = os.getcwd()\n",
    "model_path = os.path.join(pwd, \"models\", MODEL_NAME)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "print(f\"Directory created: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17e35ec7-c653-47fa-9617-51221094bcb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def surface_segment_data_preparation(word_dictionary: dict):\n",
    "    \"\"\"\n",
    "    Generate features for surface segmentation\n",
    "    :param word_dictionary: Dictionary with words as keys and their BMES labels as values\n",
    "    :return: Features (X), Labels (Y), and word characters\n",
    "    \"\"\"\n",
    "    X = []  # Features for each word\n",
    "    Y = []  # Labels for each word\n",
    "    words = []  # Original words\n",
    "    \n",
    "    for word, label in word_dictionary.items():\n",
    "        if len(word) != len(label):\n",
    "            warnings.warn(f\"Skipping word {word} due to length mismatch with label {label}\")\n",
    "            continue\n",
    "            \n",
    "        word_features = []  # Features for each character in word\n",
    "        \n",
    "        for i in range(len(word)):\n",
    "            features = {}\n",
    "            \n",
    "            # Basic character features\n",
    "            features[\"char\"] = word[i]\n",
    "            features[\"lower\"] = word[i].lower()\n",
    "            \n",
    "            # Position features\n",
    "            features[\"start\"] = i == 0\n",
    "            features[\"end\"] = i == len(word) - 1\n",
    "            features[\"position\"] = i\n",
    "            features[\"word_length\"] = len(word)\n",
    "            \n",
    "            # Context window features (larger window)\n",
    "            for j in range(-3, 4):  # -3 to +3 window\n",
    "                if 0 <= i + j < len(word):\n",
    "                    features[f\"char_{j}\"] = word[i + j]\n",
    "                    features[f\"is_vowel_{j}\"] = word[i + j].lower() in 'aeiou'\n",
    "            \n",
    "            # N-gram features\n",
    "            for n in range(1, 4):  # Unigrams, bigrams, and trigrams\n",
    "                # Previous n-grams\n",
    "                if i >= n:\n",
    "                    features[f\"prev_{n}gram\"] = word[i-n:i]\n",
    "                # Next n-grams\n",
    "                if i + n <= len(word):\n",
    "                    features[f\"next_{n}gram\"] = word[i:i+n]\n",
    "            \n",
    "            # Character type features\n",
    "            features[\"is_vowel\"] = word[i].lower() in 'aeiou'\n",
    "            features[\"is_consonant\"] = word[i].lower() not in 'aeiou'\n",
    "            \n",
    "            # Complex pattern features\n",
    "            if i > 0:\n",
    "                features[\"prev_is_vowel\"] = word[i-1].lower() in 'aeiou'\n",
    "                features[\"char_pair\"] = word[i-1:i+1]\n",
    "            if i < len(word) - 1:\n",
    "                features[\"next_is_vowel\"] = word[i+1].lower() in 'aeiou'\n",
    "            \n",
    "            # Add syllable-like features\n",
    "            if i > 0 and i < len(word) - 1:\n",
    "                prev_char = word[i-1].lower()\n",
    "                curr_char = word[i].lower()\n",
    "                next_char = word[i+1].lower()\n",
    "                features[\"syllable_pattern\"] = (\n",
    "                    (\"V\" if prev_char in 'aeiou' else \"C\") +\n",
    "                    (\"V\" if curr_char in 'aeiou' else \"C\") +\n",
    "                    (\"V\" if next_char in 'aeiou' else \"C\")\n",
    "                )\n",
    "            \n",
    "            word_features.append(features)\n",
    "        \n",
    "        X.append(word_features)\n",
    "        Y.append(list(label))\n",
    "        words.append([char for char in word])\n",
    "    \n",
    "    return X, Y, words\n",
    "    \n",
    "def evaluate_surface_segmentation(Y_pred, Y_true, test_tokens, test_segments):\n",
    "    \"\"\"\n",
    "    Evaluate surface segmentation performance\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for token, pred_labels in zip(test_tokens, Y_pred):\n",
    "        # Convert BMES labels to segmentation\n",
    "        segmented = []\n",
    "        for char, label in zip(token, pred_labels):\n",
    "            segmented.append(char)\n",
    "            if label in ['E', 'S']:\n",
    "                segmented.append('-')\n",
    "        pred_segmented = ''.join(segmented).rstrip('-')\n",
    "        predictions.append(pred_segmented)\n",
    "\n",
    "\n",
    "    return predictions, test_segments\n",
    "\n",
    "        \n",
    "def surface_labelled_data_preparation(word_dictionary: dict):\n",
    "    \"\"\"\n",
    "    Generate features for segment labelling\n",
    "    :param word_dictionary: Dictionary with segmented words as keys and their labels as values\n",
    "    :return: Features (X), Labels (Y), and words\n",
    "    \"\"\"\n",
    "    X = []  # Features for each word\n",
    "    Y = []  # Labels for each word\n",
    "    words = []  # Original words\n",
    "    \n",
    "    for word, labels in word_dictionary.items():\n",
    "        segments = word.split('-')\n",
    "        label_segments = labels.split('-')\n",
    "        \n",
    "        if len(segments) != len(label_segments):\n",
    "            warnings.warn(f\"Skipping {word} due to segment/label mismatch\")\n",
    "            continue\n",
    "            \n",
    "        segment_features = []\n",
    "        \n",
    "        for i, segment in enumerate(segments):\n",
    "            features = {}\n",
    "            \n",
    "            # Basic segment features\n",
    "            features['segment'] = segment\n",
    "            features['length'] = len(segment)\n",
    "            features['position'] = str(i)\n",
    "            features['position_pct'] = str(i / len(segments))\n",
    "            \n",
    "            # Lexical features\n",
    "            features['segment.lower()'] = segment.lower()\n",
    "            features['prefix_3'] = segment[:3] if len(segment) >= 3 else segment\n",
    "            features['suffix_3'] = segment[-3:] if len(segment) >= 3 else segment\n",
    "            \n",
    "            # Character type features\n",
    "            features['has_vowel'] = any(c.lower() in 'aeiou' for c in segment)\n",
    "            features['all_consonants'] = all(c.lower() not in 'aeiou' for c in segment)\n",
    "            features['has_upper'] = any(c.isupper() for c in segment)\n",
    "            features['is_single_char'] = len(segment) == 1\n",
    "            \n",
    "            # Context features\n",
    "            if i > 0:\n",
    "                features['prev_segment'] = segments[i-1]\n",
    "                features['prev_length'] = len(segments[i-1])\n",
    "            if i < len(segments) - 1:\n",
    "                features['next_segment'] = segments[i+1]\n",
    "                features['next_length'] = len(segments[i+1])\n",
    "            \n",
    "            # Pattern features\n",
    "            features['starts_with_vowel'] = segment[0].lower() in 'aeiou'\n",
    "            features['ends_with_vowel'] = segment[-1].lower() in 'aeiou'\n",
    "            features['consonant_pattern'] = ''.join('C' if c.lower() not in 'aeiou' else 'V' \n",
    "                                                  for c in segment)\n",
    "            \n",
    "            segment_features.append(features)\n",
    "        \n",
    "        X.append(segment_features)\n",
    "        Y.append(label_segments)\n",
    "        words.append(word)\n",
    "    \n",
    "    return X, Y, words\n",
    "\n",
    "def surface_labelled_data_preparation_pipeline(word_list: list):\n",
    "    \"\"\"\n",
    "    Generate features for pipeline segment labelling\n",
    "    :param word_list: List of segmented words\n",
    "    :return: List of features for each segmented word\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        segments = word.split('-')\n",
    "        segment_features = []\n",
    "        \n",
    "        for i, segment in enumerate(segments):\n",
    "            features = {}\n",
    "            \n",
    "            # Basic segment features\n",
    "            features['segment'] = segment\n",
    "            features['length'] = len(segment)\n",
    "            features['position'] = str(i)\n",
    "            features['position_pct'] = str(i / len(segments))\n",
    "            \n",
    "            # Lexical features\n",
    "            features['segment.lower()'] = segment.lower()\n",
    "            features['prefix_3'] = segment[:3] if len(segment) >= 3 else segment\n",
    "            features['suffix_3'] = segment[-3:] if len(segment) >= 3 else segment\n",
    "            \n",
    "            # Character type features\n",
    "            features['has_vowel'] = any(c.lower() in 'aeiou' for c in segment)\n",
    "            features['all_consonants'] = all(c.lower() not in 'aeiou' for c in segment)\n",
    "            features['has_upper'] = any(c.isupper() for c in segment)\n",
    "            features['is_single_char'] = len(segment) == 1\n",
    "            \n",
    "            # Context features\n",
    "            if i > 0:\n",
    "                features['prev_segment'] = segments[i-1]\n",
    "                features['prev_length'] = len(segments[i-1])\n",
    "            if i < len(segments) - 1:\n",
    "                features['next_segment'] = segments[i+1]\n",
    "                features['next_length'] = len(segments[i+1])\n",
    "            \n",
    "            # Pattern features\n",
    "            features['starts_with_vowel'] = segment[0].lower() in 'aeiou'\n",
    "            features['ends_with_vowel'] = segment[-1].lower() in 'aeiou'\n",
    "            features['consonant_pattern'] = ''.join('C' if c.lower() not in 'aeiou' else 'V' \n",
    "                                                  for c in segment)\n",
    "            \n",
    "            segment_features.append(features)\n",
    "            \n",
    "        X.append(segment_features)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def save_evaluation_results(model_path, position_scores, bleu_scores, chrf_score):\n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'Position Sensitive Metrics': {\n",
    "            'Precision': position_scores['precision'],\n",
    "            'Recall': position_scores['recall'],\n",
    "            'F1': position_scores['f1']\n",
    "        },\n",
    "        'BLEU Scores': {\n",
    "            'Unigram': bleu_scores['unigram'],\n",
    "            'Bigram': bleu_scores['bigram'],\n",
    "            'Equal Weights': bleu_scores['equal']\n",
    "        },\n",
    "        'chrF Score': chrf_score\n",
    "    }\n",
    "    \n",
    "    # Save to text file\n",
    "    with open(f'{model_path}/evaluation_results.txt', 'w') as f:\n",
    "        \n",
    "        f.write(f\"=== Morphological Segmentation Evaluation Results CRF - {MODEL_NAME} ===\\n\\n\")\n",
    "        \n",
    "        # Position sensitive scores\n",
    "        f.write(\"Position-sensitive scores:\\n\")\n",
    "        f.write(f\"Precision: {results['Position Sensitive Metrics']['Precision']:.3f}\\n\")\n",
    "        f.write(f\"Recall: {results['Position Sensitive Metrics']['Recall']:.3f}\\n\")\n",
    "        f.write(f\"F1: {results['Position Sensitive Metrics']['F1']:.3f}\\n\\n\")\n",
    "        \n",
    "        # BLEU scores\n",
    "        f.write(\"BLEU Scores:\\n\")\n",
    "        f.write(f\"Unigram only: {results['BLEU Scores']['Unigram']:.4f}\\n\")\n",
    "        f.write(f\"Bigram only: {results['BLEU Scores']['Bigram']:.4f}\\n\")\n",
    "        f.write(f\"Equal weights: {results['BLEU Scores']['Equal Weights']:.4f}\\n\\n\")\n",
    "        \n",
    "        # chrF score\n",
    "        f.write(f\"chrF Score: {results['chrF Score']:.4f}\\n\")\n",
    "\n",
    "def eval_morph_segments_position(predicted, target):\n",
    "\n",
    "    predicted= [word.split('-') for word in predicted]\n",
    "    target = [word.split('-') for word in target]\n",
    "    \n",
    "    correct = 0.0\n",
    "    assert len(predicted)==len(target)\n",
    "    \n",
    "    # Iterate through predicted and target words\n",
    "    for pred, targ in zip(predicted, target):\n",
    "        # Create enumerated pairs to track position\n",
    "        pred_with_pos = list(enumerate(pred))\n",
    "        targ_with_pos = list(enumerate(targ))\n",
    "        \n",
    "        # Check matches at same positions\n",
    "        for pos, p_morph in pred_with_pos:\n",
    "            # Look for match at same position in target\n",
    "            if pos < len(targ) and p_morph == targ[pos]:\n",
    "                correct += 1\n",
    "    \n",
    "    predicted_length = sum([len(pred) for pred in predicted])\n",
    "    target_length = sum([len(targ) for targ in target])\n",
    "    \n",
    "    precision = correct/predicted_length\n",
    "    recall = correct/target_length\n",
    "    f_score = 2 * (precision * recall)/(precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(\"Position-sensitive scores:\")\n",
    "    print(\"P: \", round(precision*100,3),\n",
    "          \"R: \", round(recall*100,3),\n",
    "          \"F1: \", round(f_score*100,3))\n",
    "    \n",
    "    # Return scores as dictionary\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f_score\n",
    "    }\n",
    "\n",
    "def eval_bleu_segment(predicted, targeted):\n",
    "    reference = [[word.split('-')] for word in targeted]\n",
    "    candidate = [word.split('-') for word in predicted]\n",
    "    \n",
    "    smoothie = SmoothingFunction().method2\n",
    "    \n",
    "    bleu_scores = {\n",
    "        'unigram': corpus_bleu(reference, candidate, \n",
    "                              weights=(1, 0, 0, 0), \n",
    "                              smoothing_function=smoothie),\n",
    "        'bigram': corpus_bleu(reference, candidate, \n",
    "                             weights=(0, 1, 0, 0), \n",
    "                             smoothing_function=smoothie),\n",
    "        'equal': corpus_bleu(reference, candidate, \n",
    "                            weights=(0.5, 0.5, 0, 0), \n",
    "                            smoothing_function=smoothie)\n",
    "    }\n",
    "\n",
    "    return bleu_scores\n",
    "\n",
    "def eval_chrF_segment(predicted, targeted):\n",
    "\n",
    "    target = [\" \".join(word.split(\"-\")) for word in targeted]\n",
    "    predicted = [\" \".join(word.split(\"-\")) for word in predicted]\n",
    "    \n",
    "    chrf_score = corpus_chrf(target, predicted, min_len=1, max_len=6, beta=3.0)\n",
    "\n",
    "    return chrf_score\n",
    "\n",
    "\n",
    "class BaselineCRF:\n",
    "    \"\"\"\n",
    "    Modified Baseline CRF to work with CSV data containing token and segmentation pairs\n",
    "    \"\"\"\n",
    "    def _get_unsegmented_token(self, segmented):\n",
    "        \"\"\"\n",
    "        Generate unsegmented token from segmented form\n",
    "        \"\"\"\n",
    "        return ''.join(segmented.split('-'))\n",
    "\n",
    "    def _generate_bmes_labels(self, row):\n",
    "        \"\"\"\n",
    "        Generate BMES labels for a token based on its segmentation\n",
    "        \n",
    "        :param row: DataFrame row containing segmented form\n",
    "        :return: String of BMES labels\n",
    "        \"\"\"\n",
    "        segmented = row[MODEL_NAME]\n",
    "        # Generate the actual token by removing hyphens\n",
    "        token = self._get_unsegmented_token(segmented)\n",
    "        segments = segmented.split('-')\n",
    "        \n",
    "        # Initialize empty label string\n",
    "        labels = ''\n",
    "        current_pos = 0\n",
    "        \n",
    "        # Generate labels for each segment\n",
    "        for segment in segments:\n",
    "            segment_len = len(segment)\n",
    "            \n",
    "            # Skip empty segments\n",
    "            if segment_len == 0:\n",
    "                continue\n",
    "                \n",
    "            # Single character segment\n",
    "            if segment_len == 1:\n",
    "                labels += 'S'\n",
    "            # Multi-character segment\n",
    "            else:\n",
    "                labels += 'B'  # Beginning\n",
    "                for _ in range(segment_len - 2):\n",
    "                    labels += 'M'  # Middle\n",
    "                labels += 'E'  # End\n",
    "            \n",
    "            current_pos += segment_len\n",
    "        \n",
    "        # Verify label length matches token length\n",
    "        if len(labels) != len(token):\n",
    "            print(f\"Mismatch for segmentation: {segmented}\")\n",
    "            print(f\"Generated labels: {labels}\")\n",
    "            print(f\"Unsegmented token: {token}\")\n",
    "            print(f\"Label length: {len(labels)}, Token length: {len(token)}\\n\")\n",
    "            return None\n",
    "            \n",
    "        return labels\n",
    "\n",
    "    def _preprocess_dataframe(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess the dataframe to ensure data quality\n",
    "        \"\"\"\n",
    "        # Clean up the data\n",
    "        df = df.copy()\n",
    "        df[MODEL_NAME] = df[MODEL_NAME].str.strip()\n",
    "        \n",
    "        # Generate tokens from segmentations\n",
    "        df['tokens'] = df[MODEL_NAME].apply(self._get_unsegmented_token)\n",
    "        \n",
    "        # Generate BMES labels\n",
    "        df['bmes_labels'] = df.apply(self._generate_bmes_labels, axis=1)\n",
    "        \n",
    "        # Remove rows where label generation failed\n",
    "        df = df.dropna(subset=['bmes_labels'])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the CRF with a CSV file containing tokens and their segmentations\n",
    "        \n",
    "        :param file_path: Path to the CSV file containing the data\n",
    "        \"\"\"\n",
    "        # Read the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, header=None, names=['tokens', MODEL_NAME]).head(200000)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file: {str(e)}\")\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df = self._preprocess_dataframe(df)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No valid data remains after preprocessing\")\n",
    "            \n",
    "        print(f\"Total valid samples after preprocessing: {len(df)}\")\n",
    "\n",
    "        \n",
    "        # Split into train (70%), dev (15%), and test (15%)\n",
    "        train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        dev_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Convert to dictionary format {token: bmes_labels}\n",
    "        self.training_data = dict(zip(train_data['tokens'], train_data['bmes_labels']))\n",
    "        self.dev_data = dict(zip(dev_data['tokens'], dev_data['bmes_labels']))\n",
    "        self.test_data = dict(zip(test_data['tokens'], test_data['bmes_labels']))\n",
    "        \n",
    "        # Store dictionaries in a list for easy access\n",
    "        self.data_splits = [self.training_data, self.dev_data, self.test_data]\n",
    "        \n",
    "        # Store original segmentations for evaluation\n",
    "        self.training_segments = dict(zip(train_data['tokens'], train_data[MODEL_NAME]))\n",
    "        self.dev_segments = dict(zip(dev_data['tokens'], dev_data[MODEL_NAME]))\n",
    "        self.test_segments = dict(zip(test_data['tokens'], test_data[MODEL_NAME]))\n",
    "\n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"Training samples: {len(self.training_data)}\")\n",
    "        print(f\"Development samples: {len(self.dev_data)}\")\n",
    "        print(f\"Test samples: {len(self.test_data)}\")\n",
    "\n",
    "    def train_surface_model(self, use_grid_search=True):\n",
    "        X_training, Y_training, _ = surface_segment_data_preparation(self.training_data)\n",
    "        X_dev, Y_dev, _ = surface_segment_data_preparation(self.dev_data)\n",
    "        \n",
    "        if use_grid_search:\n",
    "            param_grid = {\n",
    "                'c1': [0.1, 0.2, 0.3, 0.4],\n",
    "                'c2': [0.1, 0.2, 0.3, 0.4],\n",
    "                'max_iterations': [100, 300, 500],\n",
    "                'min_freq': [2, 3, 4]\n",
    "            }\n",
    "            \n",
    "            crf = sklearn_crfsuite.CRF(\n",
    "                algorithm='lbfgs',\n",
    "                all_possible_transitions=True\n",
    "            )\n",
    "            \n",
    "            f1_scorer = make_scorer(metrics.flat_f1_score, average='weighted')\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=crf,\n",
    "                param_grid=param_grid,\n",
    "                scoring=f1_scorer,\n",
    "                cv=2,\n",
    "                verbose=1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            print(\"Running grid search...\")\n",
    "            grid_search.fit(X_training, Y_training)\n",
    "            print(\"Best parameters:\", grid_search.best_params_)\n",
    "            print(\"Best F1 score:\", grid_search.best_score_)\n",
    "            \n",
    "            return grid_search.best_estimator_\n",
    "        else:\n",
    "\n",
    "            print(\"using the vanilla model\")\n",
    "            \n",
    "            crf = sklearn_crfsuite.CRF(\n",
    "                algorithm='lbfgs',\n",
    "                c1=0.2,\n",
    "                c2=0.3,\n",
    "                max_iterations=500,\n",
    "                all_possible_transitions=True,\n",
    "                min_freq=3,\n",
    "                verbose=True\n",
    "            )\n",
    "            crf.fit(X_training, Y_training)\n",
    "            return crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8822144-756d-494e-bfd9-6fd2de631bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/valid_linearizations_2.csv'\n",
    "\n",
    "# Initialize the CRF\n",
    "crf = BaselineCRF(file_path)\n",
    "print(\"CRF model initialized successfully\")\n",
    "\n",
    "# Train new model\n",
    "print(\"Training improved model...\")\n",
    "surface_model = crf.train_surface_model(use_grid_search=False)\n",
    "\n",
    "# Get test data\n",
    "test_tokens = list(crf.test_data.keys())\n",
    "test_segments = list(crf.test_segments.values())\n",
    "\n",
    "# Make predictions with new model\n",
    "X_test, Y_test, _ = surface_segment_data_preparation(crf.test_data)\n",
    "Y_pred = surface_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b587e868-a132-42b8-b381-f4593bc3f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "predicted, targeted = evaluate_surface_segmentation(Y_pred, Y_test, test_tokens, test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef3adb5e-6152-4133-908c-ec025c8bc09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position-sensitive scores:\n",
      "P:  78.556 R:  79.15 F1:  78.852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.785560745397831,\n",
       " 'recall': 0.7914963604683353,\n",
       " 'f1': 0.788517382910725}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P, F, R, BASED\n",
    "pos_scores = eval_morph_segments_position(predicted, targeted)\n",
    "pos_scores\n",
    "\n",
    "bleu_scores = eval_bleu_segment(predicted, targeted)\n",
    "bleu_scores\n",
    "\n",
    "chrf_score = eval_chrF_segment(predicted, targeted)\n",
    "chrf_score\n",
    "\n",
    "save_evaluation_results(model_path, pos_scores, bleu_scores, chrf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fadc90d-ec93-4d45-adc9-026a8d9f102f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unigram': 0.8470738448111454,\n",
       " 'bigram': 0.7757422021796317,\n",
       " 'equal': 0.8106237905357612}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores = eval_bleu_segment(predicted, targeted)\n",
    "bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3cc4ea2-4bb3-44e7-bba0-8df8219d6cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968397291196388"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrf_score = eval_chrF_segment(predicted, targeted)\n",
    "chrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee617b46-aae4-4e44-908b-58b30874d694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_evaluation_results(model_path, pos_scores, bleu_scores, chrf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a49b14c-d370-4458-aaa6-0336a9232880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model successfully saved to /workspace/segmentation-models/crf_model/models/segmenter_three/segmenter_three.pkl\n",
      "File size: 0.78 MB\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, filepath='zulu_morphological_segmenter.pkl'):\n",
    "    \"\"\"\n",
    "    Save the trained CRF model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CRF model\n",
    "        filepath: Path where to save the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model successfully saved to {filepath}\")\n",
    "        print(f\"File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "def load_model(filepath='zulu_morphological_segmenter.pkl'):\n",
    "    \"\"\"\n",
    "    Load a saved CRF model\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the saved model\n",
    "    Returns:\n",
    "        Loaded CRF model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model successfully loaded from {filepath}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "save_model(surface_model, f\"{model_path}/{MODEL_NAME}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c06625-7c48-4fd7-9173-36dad24de16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Specify your CSV file path\n",
    "#     file_path = '../data/unique_sorted_validated_words_newer_v2.csv'\n",
    "    \n",
    "#     # Initialize the CRF\n",
    "#     try:\n",
    "#         crf = BaselineCRF(file_path)\n",
    "#         print(\"CRF model initialized successfully\")\n",
    "\n",
    "#         # Train new model\n",
    "#         print(\"Training improved model...\")\n",
    "#         surface_model = crf.train_surface_model(crf.training_data)\n",
    "        \n",
    "#         # Get test data\n",
    "#         test_tokens = list(crf.test_data.keys())\n",
    "#         test_segments = list(crf.test_segments.values())\n",
    "\n",
    "#         # Make predictions with new model\n",
    "#         X_test, Y_test, _ = crf.surface_segment_data_preparation(crf.test_data)\n",
    "#         Y_pred = surface_model.predict(X_test)\n",
    "        \n",
    "#         # Evaluate\n",
    "#         evaluate_surface_segmentation(Y_pred, Y_test, test_tokens, test_segments)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error initializing CRF model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21debc3b-4ca4-4b8f-bf66-3e2bd64f4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For parameter tuning...\n",
    "\n",
    "# file_path = '../data/valid_linearizations_2.csv'\n",
    "# model = BaselineCRF(file_path)\n",
    "# crf = model.train_surface_model(use_grid_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "012b3ef9-9e47-48aa-9b87-762c4906f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_word_segmentation(surface_model, word):\n",
    "#     \"\"\"\n",
    "#     Predict segmentation for a custom word\n",
    "    \n",
    "#     Args:\n",
    "#         surface_model: Trained CRF model\n",
    "#         word: Word to segment\n",
    "#     Returns:\n",
    "#         Segmented version of the word\n",
    "#     \"\"\"\n",
    "#     # Prepare features\n",
    "#     features = []\n",
    "#     for i in range(len(word)):\n",
    "#         char_features = {}\n",
    "        \n",
    "#         # Basic character features\n",
    "#         char_features[\"char\"] = word[i]\n",
    "#         char_features[\"lower\"] = word[i].lower()\n",
    "        \n",
    "#         # Position features\n",
    "#         char_features[\"start\"] = i == 0\n",
    "#         char_features[\"end\"] = i == len(word) - 1\n",
    "#         char_features[\"position\"] = i\n",
    "#         char_features[\"word_length\"] = len(word)\n",
    "        \n",
    "#         # Context window features\n",
    "#         for j in range(-3, 4):  # -3 to +3 window\n",
    "#             if 0 <= i + j < len(word):\n",
    "#                 char_features[f\"char_{j}\"] = word[i + j]\n",
    "#                 char_features[f\"is_vowel_{j}\"] = word[i + j].lower() in 'aeiou'\n",
    "        \n",
    "#         # N-gram features\n",
    "#         for n in range(1, 4):\n",
    "#             # Previous n-grams\n",
    "#             if i >= n:\n",
    "#                 char_features[f\"prev_{n}gram\"] = word[i-n:i]\n",
    "#             # Next n-grams\n",
    "#             if i + n <= len(word):\n",
    "#                 char_features[f\"next_{n}gram\"] = word[i:i+n]\n",
    "        \n",
    "#         # Character type features\n",
    "#         char_features[\"is_vowel\"] = word[i].lower() in 'aeiou'\n",
    "#         char_features[\"is_consonant\"] = word[i].lower() not in 'aeiou'\n",
    "        \n",
    "#         # Complex pattern features\n",
    "#         if i > 0:\n",
    "#             char_features[\"prev_is_vowel\"] = word[i-1].lower() in 'aeiou'\n",
    "#             char_features[\"char_pair\"] = word[i-1:i+1]\n",
    "#         if i < len(word) - 1:\n",
    "#             char_features[\"next_is_vowel\"] = word[i+1].lower() in 'aeiou'\n",
    "        \n",
    "#         # Syllable-like features\n",
    "#         if i > 0 and i < len(word) - 1:\n",
    "#             prev_char = word[i-1].lower()\n",
    "#             curr_char = word[i].lower()\n",
    "#             next_char = word[i+1].lower()\n",
    "#             char_features[\"syllable_pattern\"] = (\n",
    "#                 (\"V\" if prev_char in 'aeiou' else \"C\") +\n",
    "#                 (\"V\" if curr_char in 'aeiou' else \"C\") +\n",
    "#                 (\"V\" if next_char in 'aeiou' else \"C\")\n",
    "#             )\n",
    "        \n",
    "#         features.append(char_features)\n",
    "    \n",
    "#     # Get predictions\n",
    "#     predictions = surface_model.predict([features])[0]\n",
    "    \n",
    "#     # Convert to segmented form\n",
    "#     segmented = []\n",
    "#     for char, label in zip(word, predictions):\n",
    "#         segmented.append(char)\n",
    "#         if label in ['E', 'S']:\n",
    "#             segmented.append('-')\n",
    "    \n",
    "#     return ''.join(segmented).rstrip('-')\n",
    "\n",
    "# # Example usage:\n",
    "# # Test with some custom words\n",
    "# test_words = [\n",
    "#     \"ngiyabonga\",\n",
    "#     \"uyajabula\",\n",
    "#     \"sizohamba\",\n",
    "#     \"ngizokuthanda\"\n",
    "# ]\n",
    "\n",
    "# print(\"Testing custom words:\")\n",
    "# for word in test_words:\n",
    "#     segmented = predict_word_segmentation(surface_model, word)\n",
    "#     print(f\"Word: {word}\")\n",
    "#     print(f\"Predicted segmentation: {segmented}\")\n",
    "#     print()\n",
    "\n",
    "# # Interactive testing\n",
    "# def interactive_testing():\n",
    "#     print(\"\\nEnter words to segment (type 'quit' to exit):\")\n",
    "#     while True:\n",
    "#         word = input(\"\\nEnter word: \").strip()\n",
    "#         if word.lower() == 'quit':\n",
    "#             break\n",
    "#         if word:\n",
    "#             segmented = predict_word_segmentation(surface_model, word)\n",
    "#             print(f\"Predicted segmentation: {segmented}\")\n",
    "\n",
    "# # Run interactive testing\n",
    "# interactive_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b0c04b7-12fe-4da2-9d30-199ff6c21623",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Test loading the model\n",
    "\n",
    "# print(\"\\nTesting model loading...\")\n",
    "# loaded_model = load_model(f\"{model_path}/{MODEL_NAME}.pkl\")\n",
    "\n",
    "# if loaded_model:\n",
    "#     print(\"\\nTesting loaded model with a sample word...\")\n",
    "#     test_word = \"ngiyabonga\"\n",
    "#     segmented = predict_word_segmentation(loaded_model, test_word)\n",
    "#     print(f\"Word: {test_word}\")\n",
    "#     print(f\"Predicted segmentation: {segmented}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
