{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb1483-3a83-408b-9134-d78757ae03a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.18.0 at http://b378ea036eb9:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=\"../pytorch-transformer_2/translation_trial/zulu_english_segmenter_two/tensor_data\" --bind_all --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b4e8b9-aaf7-4714-ae89-658825f15299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "  Installing build dependencies ... \u001bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /root/miniconda3/envs/ml/lib/python3.10/site-packages (from fasttext) (72.1.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/ml/lib/python3.10/site-packages (from fasttext) (1.24.1)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pydone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=325277 sha256=fdd7b339ff58ecf4ecef5c066af2a2977d73ffe3ca737e67032348ccbdc2a777\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "677faea2-8f88-48f1-b5b5-57267a9a532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tsn_Latn'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import fasttext\n",
    "import re\n",
    "import csv\n",
    "\n",
    "model = fasttext.load_model('lid201-model.bin')\n",
    "lang_prediction = model.predict(\"ke a mo rata\")\n",
    "lang_label = lang_prediction[0][0].split(\"__label__\")[1]\n",
    "lang_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fbd7014-579c-4aeb-a8b9-8b5ed9dc0a6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 1000, Accepted: 863, Unique Zulu: 863\n",
      "Processed: 2000, Accepted: 1740, Unique Zulu: 1740\n",
      "Processed: 3000, Accepted: 2631, Unique Zulu: 2631\n",
      "Processed: 4000, Accepted: 3507, Unique Zulu: 3507\n",
      "Processed: 5000, Accepted: 4408, Unique Zulu: 4408\n",
      "Processed: 6000, Accepted: 5310, Unique Zulu: 5310\n",
      "Processed: 7000, Accepted: 6204, Unique Zulu: 6204\n",
      "Processed: 8000, Accepted: 7109, Unique Zulu: 7109\n",
      "Processed: 9000, Accepted: 8003, Unique Zulu: 8003\n",
      "Processed: 10000, Accepted: 8889, Unique Zulu: 8889\n",
      "Processed: 11000, Accepted: 9773, Unique Zulu: 9773\n",
      "Processed: 12000, Accepted: 10664, Unique Zulu: 10664\n",
      "Processed: 13000, Accepted: 11561, Unique Zulu: 11561\n",
      "Processed: 14000, Accepted: 12456, Unique Zulu: 12456\n",
      "Processed: 15000, Accepted: 13356, Unique Zulu: 13356\n",
      "Processed: 16000, Accepted: 14265, Unique Zulu: 14265\n",
      "Processed: 17000, Accepted: 15140, Unique Zulu: 15140\n",
      "Processed: 18000, Accepted: 16027, Unique Zulu: 16027\n",
      "Processed: 19000, Accepted: 16910, Unique Zulu: 16910\n",
      "Processed: 20000, Accepted: 17786, Unique Zulu: 17786\n",
      "Processed: 21000, Accepted: 18666, Unique Zulu: 18666\n",
      "Processed: 22000, Accepted: 19559, Unique Zulu: 19559\n",
      "Processed: 23000, Accepted: 20426, Unique Zulu: 20426\n",
      "Processed: 24000, Accepted: 21306, Unique Zulu: 21306\n",
      "Processed: 25000, Accepted: 22178, Unique Zulu: 22178\n",
      "Processed: 26000, Accepted: 23064, Unique Zulu: 23064\n",
      "Processed: 27000, Accepted: 23948, Unique Zulu: 23948\n",
      "Processed: 28000, Accepted: 24816, Unique Zulu: 24816\n",
      "Processed: 29000, Accepted: 25710, Unique Zulu: 25710\n",
      "Processed: 30000, Accepted: 26574, Unique Zulu: 26574\n",
      "Processed: 31000, Accepted: 27445, Unique Zulu: 27445\n",
      "Processed: 32000, Accepted: 28300, Unique Zulu: 28300\n",
      "Processed: 33000, Accepted: 29145, Unique Zulu: 29145\n",
      "Processed: 34000, Accepted: 30016, Unique Zulu: 30016\n",
      "Processed: 35000, Accepted: 30895, Unique Zulu: 30895\n",
      "Processed: 36000, Accepted: 31738, Unique Zulu: 31738\n",
      "Processed: 37000, Accepted: 32611, Unique Zulu: 32611\n",
      "Processed: 38000, Accepted: 33482, Unique Zulu: 33482\n",
      "Processed: 39000, Accepted: 34330, Unique Zulu: 34330\n",
      "Processed: 40000, Accepted: 35184, Unique Zulu: 35184\n",
      "Processed: 41000, Accepted: 36036, Unique Zulu: 36036\n",
      "Processed: 42000, Accepted: 36901, Unique Zulu: 36901\n",
      "Processed: 43000, Accepted: 37760, Unique Zulu: 37760\n",
      "Processed: 44000, Accepted: 38628, Unique Zulu: 38628\n",
      "Processed: 45000, Accepted: 39499, Unique Zulu: 39499\n",
      "Processed: 46000, Accepted: 40374, Unique Zulu: 40374\n",
      "Processed: 47000, Accepted: 41212, Unique Zulu: 41212\n",
      "Processed: 48000, Accepted: 42065, Unique Zulu: 42065\n",
      "Processed: 49000, Accepted: 42937, Unique Zulu: 42937\n",
      "Processed: 50000, Accepted: 43773, Unique Zulu: 43773\n",
      "Processed: 51000, Accepted: 44618, Unique Zulu: 44618\n",
      "Processed: 52000, Accepted: 45457, Unique Zulu: 45457\n",
      "Processed: 53000, Accepted: 46293, Unique Zulu: 46293\n",
      "Processed: 54000, Accepted: 47132, Unique Zulu: 47132\n",
      "Processed: 55000, Accepted: 47986, Unique Zulu: 47986\n",
      "Processed: 56000, Accepted: 48829, Unique Zulu: 48829\n",
      "Processed: 57000, Accepted: 49665, Unique Zulu: 49665\n",
      "Processed: 58000, Accepted: 50497, Unique Zulu: 50497\n",
      "Processed: 59000, Accepted: 51335, Unique Zulu: 51335\n",
      "Processed: 60000, Accepted: 52187, Unique Zulu: 52187\n",
      "Processed: 61000, Accepted: 53019, Unique Zulu: 53019\n",
      "Processed: 62000, Accepted: 53867, Unique Zulu: 53867\n",
      "Processed: 63000, Accepted: 54720, Unique Zulu: 54720\n",
      "Processed: 64000, Accepted: 55557, Unique Zulu: 55557\n",
      "Processed: 65000, Accepted: 56391, Unique Zulu: 56391\n",
      "Processed: 66000, Accepted: 57235, Unique Zulu: 57235\n",
      "Processed: 67000, Accepted: 58068, Unique Zulu: 58068\n",
      "Processed: 68000, Accepted: 58906, Unique Zulu: 58906\n",
      "Processed: 69000, Accepted: 59743, Unique Zulu: 59743\n",
      "Processed: 70000, Accepted: 60578, Unique Zulu: 60578\n",
      "Processed: 71000, Accepted: 61411, Unique Zulu: 61411\n",
      "Processed: 72000, Accepted: 62244, Unique Zulu: 62244\n",
      "Processed: 73000, Accepted: 63065, Unique Zulu: 63065\n",
      "Processed: 74000, Accepted: 63885, Unique Zulu: 63885\n",
      "Processed: 75000, Accepted: 64707, Unique Zulu: 64707\n",
      "Processed: 76000, Accepted: 65515, Unique Zulu: 65515\n",
      "Processed: 77000, Accepted: 66348, Unique Zulu: 66348\n",
      "Processed: 78000, Accepted: 67169, Unique Zulu: 67169\n",
      "Processed: 79000, Accepted: 67977, Unique Zulu: 67977\n",
      "Processed: 80000, Accepted: 68796, Unique Zulu: 68796\n",
      "Processed: 81000, Accepted: 69604, Unique Zulu: 69604\n",
      "Processed: 82000, Accepted: 70434, Unique Zulu: 70434\n",
      "Processed: 83000, Accepted: 71268, Unique Zulu: 71268\n",
      "Processed: 84000, Accepted: 72074, Unique Zulu: 72074\n",
      "Processed: 85000, Accepted: 72892, Unique Zulu: 72892\n",
      "Processed: 86000, Accepted: 73705, Unique Zulu: 73705\n",
      "Processed: 87000, Accepted: 74528, Unique Zulu: 74528\n",
      "Processed: 88000, Accepted: 75340, Unique Zulu: 75340\n",
      "Processed: 89000, Accepted: 76153, Unique Zulu: 76153\n",
      "Processed: 90000, Accepted: 76973, Unique Zulu: 76973\n",
      "Processed: 91000, Accepted: 77797, Unique Zulu: 77797\n",
      "Processed: 92000, Accepted: 78617, Unique Zulu: 78617\n",
      "Processed: 93000, Accepted: 79412, Unique Zulu: 79412\n",
      "Processed: 94000, Accepted: 80235, Unique Zulu: 80235\n",
      "Processed: 95000, Accepted: 81017, Unique Zulu: 81017\n",
      "Processed: 96000, Accepted: 81832, Unique Zulu: 81832\n",
      "Processed: 97000, Accepted: 82641, Unique Zulu: 82641\n",
      "Processed: 98000, Accepted: 83447, Unique Zulu: 83447\n",
      "Processed: 99000, Accepted: 84225, Unique Zulu: 84225\n",
      "Processed: 100000, Accepted: 85024, Unique Zulu: 85024\n",
      "Processed: 101000, Accepted: 85834, Unique Zulu: 85834\n",
      "Processed: 102000, Accepted: 86647, Unique Zulu: 86647\n",
      "Processed: 103000, Accepted: 87439, Unique Zulu: 87439\n",
      "Processed: 104000, Accepted: 88223, Unique Zulu: 88223\n",
      "Processed: 105000, Accepted: 89042, Unique Zulu: 89042\n",
      "Processed: 106000, Accepted: 89855, Unique Zulu: 89855\n",
      "Processed: 107000, Accepted: 90657, Unique Zulu: 90657\n",
      "Processed: 108000, Accepted: 91460, Unique Zulu: 91460\n",
      "Processed: 109000, Accepted: 92256, Unique Zulu: 92256\n",
      "Processed: 110000, Accepted: 93041, Unique Zulu: 93041\n",
      "Processed: 111000, Accepted: 93825, Unique Zulu: 93825\n",
      "Processed: 112000, Accepted: 94614, Unique Zulu: 94614\n",
      "Processed: 113000, Accepted: 95407, Unique Zulu: 95407\n",
      "Processed: 114000, Accepted: 96200, Unique Zulu: 96200\n",
      "Processed: 115000, Accepted: 96982, Unique Zulu: 96982\n",
      "Processed: 116000, Accepted: 97787, Unique Zulu: 97787\n",
      "Processed: 117000, Accepted: 98584, Unique Zulu: 98584\n",
      "Processed: 118000, Accepted: 99346, Unique Zulu: 99346\n",
      "\n",
      "Final Results:\n",
      "Total pairs processed: 118799\n",
      "Accepted pairs: 100000\n",
      "Rejected pairs: 18799\n",
      "\n",
      "Results saved to:\n",
      "- Accepted pairs: accepted_pairs.csv\n",
      "- Rejected pairs: rejected_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import fasttext\n",
    "import re\n",
    "import csv\n",
    "\n",
    "model = fasttext.load_model('lid201-model.bin')\n",
    "\n",
    "def is_zulu_xhosa_swati(line):\n",
    "    lang_prediction = model.predict(line)\n",
    "    lang_label = lang_prediction[0][0].split(\"__label__\")[1]\n",
    "    if lang_label in ['zul_Latn']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_english(line):\n",
    "    lang_prediction = model.predict(line)\n",
    "    lang_label = lang_prediction[0][0].split(\"__label__\")[1]\n",
    "    if lang_label in ['eng_Latn']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_parallel_text(input_file, accepted_file, rejected_file, model, target_accepted=100000):\n",
    "    accepted_pairs = []\n",
    "    rejected_pairs = []\n",
    "    total_processed = 0\n",
    "    unique_zulu = set()  # Track unique Zulu sentences\n",
    "    \n",
    "    while len(accepted_pairs) < target_accepted:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            header = next(reader)  # Skip header\n",
    "            \n",
    "            for row in enumerate(reader):\n",
    "                # Skip rows we've already processed in previous iterations\n",
    "                if row[0] < total_processed:\n",
    "                    continue\n",
    "                    \n",
    "                if len(row[1]) != 2:\n",
    "                    continue\n",
    "                    \n",
    "                eng_text, zulu_text = row[1]\n",
    "                \n",
    "                # Check if both texts pass their respective language tests\n",
    "                valid_eng = is_english(eng_text)\n",
    "                valid_zulu = is_zulu_xhosa_swati(zulu_text)\n",
    "                is_unique_zulu = zulu_text not in unique_zulu\n",
    "                \n",
    "                # Store the pair and the reason for rejection if any\n",
    "                if valid_eng and valid_zulu and is_unique_zulu:\n",
    "                    accepted_pairs.append((eng_text, zulu_text))\n",
    "                    unique_zulu.add(zulu_text)  # Add to set of unique Zulu sentences\n",
    "                    if len(accepted_pairs) >= target_accepted:\n",
    "                        break\n",
    "                else:\n",
    "                    reason = []\n",
    "                    if not valid_eng:\n",
    "                        reason.append(\"Failed English check\")\n",
    "                    if not valid_zulu:\n",
    "                        reason.append(\"Failed Zulu/Xhosa/Swati check\")\n",
    "                    if not is_unique_zulu:\n",
    "                        reason.append(\"Duplicate Zulu sentence\")\n",
    "                    rejected_pairs.append({\n",
    "                        \"english\": eng_text,\n",
    "                        \"zulu\": zulu_text,\n",
    "                        \"reason\": \" & \".join(reason)\n",
    "                    })\n",
    "                \n",
    "                total_processed += 1\n",
    "                \n",
    "                # Print progress periodically\n",
    "                if total_processed % 1000 == 0:\n",
    "                    print(f\"Processed: {total_processed}, Accepted: {len(accepted_pairs)}, Unique Zulu: {len(unique_zulu)}\")\n",
    "        \n",
    "        # If we've processed all rows but still haven't found enough accepted pairs\n",
    "        if total_processed == sum(1 for row in csv.reader(open(input_file))) - 1:\n",
    "            print(\"Reached end of file before finding enough accepted pairs.\")\n",
    "            break\n",
    "    \n",
    "    # Write accepted pairs to CSV\n",
    "    with open(accepted_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['english', 'zulu'])\n",
    "        writer.writerows(accepted_pairs)\n",
    "    \n",
    "    # Write rejected pairs to JSON for better readability of rejection reasons\n",
    "    with open(rejected_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(rejected_pairs, outfile, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return accepted_pairs, rejected_pairs\n",
    "\n",
    "# Usage\n",
    "input_file = 'eng_zul_nllb_data.csv'\n",
    "accepted_file = 'accepted_pairs_.csv'\n",
    "rejected_file = 'rejected_pairs.json'\n",
    "\n",
    "accepted, rejected = filter_parallel_text(input_file, accepted_file, rejected_file, model)\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Total pairs processed: {len(accepted) + len(rejected)}\")\n",
    "print(f\"Accepted pairs: {len(accepted)}\")\n",
    "print(f\"Rejected pairs: {len(rejected)}\")\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"- Accepted pairs: {accepted_file}\")\n",
    "print(f\"- Rejected pairs: {rejected_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff083172-ff3c-4d90-b0a1-15709b594be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Cleaned text saved to 'flores_english_test_val.dev\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "with open(\"eng_Latn.dev\", \"r\") as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "cleaned_sentences = [preprocess_text(sentence.strip()) for sentence in sentences]\n",
    "\n",
    "with open(\"flores_english_test_val.dev\", \"w\") as file:\n",
    "    for sentence in cleaned_sentences:\n",
    "        file.write(sentence + \"\\n\")\n",
    "\n",
    "print(\"Preprocessing complete. Cleaned text saved to 'flores_english_test_val.dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9b9831-a783-462a-8346-c02c1409e9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>segmenter_three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ab ebgngilangazelela ukusondela\\n</td>\n",
       "      <td>ab ebgngi-langazelel-a uku-sondel-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aba  english ulimi yenza\\n</td>\n",
       "      <td>a-b-a engli-sh ulimi yenz-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aba adl naba florinadusayangane\\n</td>\n",
       "      <td>a-b-a a-dl nab-a florinadusay-anga-ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aba bantu badala kulindeleke ukuba bayokumela ...</td>\n",
       "      <td>a-b-a bantu ba-dal-a ku-lindelek-e uku-b-a bay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aba imiklamo yakho emkhakheni\\n</td>\n",
       "      <td>a-b-a imiklamo ya-kho emkhakheni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>zwi zwa ama zwi na khakhathi\\n</td>\n",
       "      <td>zw-i zwa ama zw-i n-a khakhathi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>zwimela zwo no nga zwikhopha\\n</td>\n",
       "      <td>zwimel-a zwo no nga zw-ikhopha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>zwisisani amazwi ami\\n</td>\n",
       "      <td>zwisis-a-ni amazwi ami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>zwothe zwa aquaponics zwine zwine zwa vha zwav...</td>\n",
       "      <td>zwoth-e zwa a-quaponics-a zwin-e zwin-e zwa vh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>zynijexogywuhaza howagemusado usutes\\n</td>\n",
       "      <td>zy-nijexogywuhaza howagemusado u-sutes-e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 original  \\\n",
       "0                       ab ebgngilangazelela ukusondela\\n   \n",
       "1                              aba  english ulimi yenza\\n   \n",
       "2                       aba adl naba florinadusayangane\\n   \n",
       "3       aba bantu badala kulindeleke ukuba bayokumela ...   \n",
       "4                         aba imiklamo yakho emkhakheni\\n   \n",
       "...                                                   ...   \n",
       "499995                     zwi zwa ama zwi na khakhathi\\n   \n",
       "499996                     zwimela zwo no nga zwikhopha\\n   \n",
       "499997                             zwisisani amazwi ami\\n   \n",
       "499998  zwothe zwa aquaponics zwine zwine zwa vha zwav...   \n",
       "499999             zynijexogywuhaza howagemusado usutes\\n   \n",
       "\n",
       "                                          segmenter_three  \n",
       "0                     ab ebgngi-langazelel-a uku-sondel-a  \n",
       "1                             a-b-a engli-sh ulimi yenz-a  \n",
       "2                   a-b-a a-dl nab-a florinadusay-anga-ne  \n",
       "3       a-b-a bantu ba-dal-a ku-lindelek-e uku-b-a bay...  \n",
       "4                        a-b-a imiklamo ya-kho emkhakheni  \n",
       "...                                                   ...  \n",
       "499995                    zw-i zwa ama zw-i n-a khakhathi  \n",
       "499996                     zwimel-a zwo no nga zw-ikhopha  \n",
       "499997                             zwisis-a-ni amazwi ami  \n",
       "499998  zwoth-e zwa a-quaponics-a zwin-e zwin-e zwa vh...  \n",
       "499999           zy-nijexogywuhaza howagemusado u-sutes-e  \n",
       "\n",
       "[500000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('./embedding_segmenter_three.csv')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea033ed-b28e-47cc-be7d-a37442220e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isizulu</th>\n",
       "      <th>segmenter_one</th>\n",
       "      <th>segmenter_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ab ebgngilangazelela ukusondela\\n</td>\n",
       "      <td>ab- e-bg-ngi-langazelel-a uku-sondel-a</td>\n",
       "      <td>ab ebgngi-langazelel-a uku-sondel-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aba  english ulimi yenza\\n</td>\n",
       "      <td>ab-a e-ng-li-sh u-limi y-enz-a</td>\n",
       "      <td>a-b-a engli-sh ulimi yenz-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aba adl naba florinadusayangane\\n</td>\n",
       "      <td>ab-a a-dl n-ab-a folorindusay-angane</td>\n",
       "      <td>a-b-a a-dl nab-a florinadusay-anga-ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aba bantu badala kulindeleke ukuba bayokumela ...</td>\n",
       "      <td>ab-a ba-ntu b-a-dal-a ku-lindelek-e uku-b-a ba...</td>\n",
       "      <td>a-b-a bantu ba-dal-a ku-lindelek-e uku-b-a bay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aba imiklamo yakho emkhakheni\\n</td>\n",
       "      <td>ab-a i-mi-klamo ya-kho e-mkhakh-eni</td>\n",
       "      <td>a-b-a imiklamo ya-kho emkhakheni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             isizulu  \\\n",
       "0                  ab ebgngilangazelela ukusondela\\n   \n",
       "1                         aba  english ulimi yenza\\n   \n",
       "2                  aba adl naba florinadusayangane\\n   \n",
       "3  aba bantu badala kulindeleke ukuba bayokumela ...   \n",
       "4                    aba imiklamo yakho emkhakheni\\n   \n",
       "\n",
       "                                       segmenter_one  \\\n",
       "0             ab- e-bg-ngi-langazelel-a uku-sondel-a   \n",
       "1                     ab-a e-ng-li-sh u-limi y-enz-a   \n",
       "2               ab-a a-dl n-ab-a folorindusay-angane   \n",
       "3  ab-a ba-ntu b-a-dal-a ku-lindelek-e uku-b-a ba...   \n",
       "4                ab-a i-mi-klamo ya-kho e-mkhakh-eni   \n",
       "\n",
       "                                       segmenter_two  \n",
       "0                ab ebgngi-langazelel-a uku-sondel-a  \n",
       "1                        a-b-a engli-sh ulimi yenz-a  \n",
       "2              a-b-a a-dl nab-a florinadusay-anga-ne  \n",
       "3  a-b-a bantu ba-dal-a ku-lindelek-e uku-b-a bay...  \n",
       "4                   a-b-a imiklamo ya-kho emkhakheni  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first CSV with English and Zulu\n",
    "df1 = pd.read_csv('./diversified_data_segmenter_one.csv')\n",
    "\n",
    "# Read the second CSV with original and segmented Zulu\n",
    "df2 = pd.read_csv('./diversified_data_segmenter_two_02.csv')\n",
    "\n",
    "\n",
    "# Read the second CSV with original and segmented Zulu\n",
    "df3 = pd.read_csv('./embedding_segmenter_three.csv')\n",
    "\n",
    "\n",
    "# Create new dataframe with desired columns\n",
    "final_df = pd.DataFrame({\n",
    "    'isizulu': df1['original'],\n",
    "    'segmenter_one': df1['segmenter_one'],\n",
    "    'segmenter_two': df2['segmenter_two'],\n",
    "    'segmenter_two': df3['segmenter_three']\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Save to new CSV\n",
    "final_df.to_csv('word_embedding_data_500.csv', index=False)\n",
    "\n",
    "# Display first few rows to verify\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0aae0f-d88f-42df-9967-47c592e15851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random sample row:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'english'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'english'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRandom sample row:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrandom_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misiZulu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmented isiZulu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegmenter_one\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'english'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"word_embedding_data_500.csv\")\n",
    "# Get a random row\n",
    "random_row = data.sample(n=1)\n",
    "\n",
    "# Print the row in a nicely formatted way\n",
    "print(\"\\nRandom sample row:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"English: {random_row['english'].values[0]}\")\n",
    "print(f\"isiZulu: {random_row['original'].values[0]}\")\n",
    "print(f\"Segmented isiZulu: {random_row['segmenter_one'].values[0]}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469db511-4b6e-4b9f-ab8d-76dcd0694f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row at index 2005\n",
      "--------------------------------------------------------------------------------\n",
      "English: high winds hail excessive precipitation and wildfires are forms and effects of severe weather as are thunderstorms tornadoes waterspouts and cyclones\n",
      "isiZulu: imimoya ephakeme isangqotho isimo sezulu esidlulele kanye nemililo yasendle ziwuhlobo nomphumela wesimo sezulu esibi kakhulu kanjalo neziphepho izivunguvungu izishomo zamanzi amuncwa yisivunguvungu kanye nezihlwithi\n",
      "\n",
      "Segmented isiZulu: i-mi-moya e-phakem-e i-s-angqotho i-si-mo se-zulu esi-dlulel-e ka-nye n-e-mi-lilo ya-sendle zi-w-u-hlobo n-o-m-phumela we-si-mo se-zulu esi-bi ka-khulu ka-njalo n-e-zi-phepho i-zi-vunguvungu i-zi-shomo za-ma-nzi a-muncw-a y-i-si-vunguvungu ka-nye n-e-zi-hlwithi\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print row at specific index (e.g., index 5)\n",
    "index = 2005  # Change this to any index you want\n",
    "row = final_df.iloc[index]\n",
    "\n",
    "print(\"\\nRow at index\", index)\n",
    "print(\"-\" * 80)\n",
    "print(f\"English: {random_row['english'].values[0]}\")\n",
    "print(f\"isiZulu: {random_row['original'].values[0]}\")\n",
    "print(f\"Segmented isiZulu: {random_row['segmenter_one'].values[0]}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7fc56066-8427-4554-9596-d361bc1d4409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read CSV file with shape: (2009, 4)\n",
      "Original CSV columns: ['original', 'segmenter_one', 'segmenter_two', 'segmenter_three']\n",
      "Read 2009 lines from text file\n",
      "\n",
      "First few rows of combined data:\n",
      "                                                                                                                                                                                                                                                                                                    isizulu                                                                                                                                                                                                                                                                                                                                                           segmenter_one                                                                                                                                                                                                                                                                                                                                                    segmenter_two                                                                                                                                                                                                                                                                                                                                segmenter_three                                                                                                                                                                                                                                                              english\n",
      "0  ngomsombuluko ososayensi basestanford university school of medicine bamemezele ithuluzi elisha elikwazi ukuhlukanisa amagqamuzane ngezinhlobo zawo ucezwana oluncane olunyathelisekayo olungakhandwa ngokusebenzisa imishini evamile yokunyathelisa ngokusesilinganisweni ngesenti elilodwa lasemelika\\n  ng-o-m-sombuluko o-sosayensi ba-s-e-stanfod u-ni-versit-ye schol-i o-f m-edicin-e ba-memezel-e i-thuluzi eli-sh-a eli-kw-azi uku-hlukanis-a a-ma-gqamuzane ng-e-zin-hlobo za-wo u-cezwan-a olu-ncane olu-nyathelisek-a-yo olu-nga-khandw-a ng-o-ku-sebenzis-a i-mi-shini e-vam-ile yo-ku-nyathelis-a ng-o-ku-s-e-si-linganis-weni ng-e-senti eli-lo-dwa l-a-s-emelik-a  ngo-m-sombuluko o-sosayensi ba-s-estanfor-do u-ni-versit-y schol-a o-f me-dicine ba-memezel-e i-thuluzi eli-sh-a eli-kw-azi uku-hlukanis-a ama-gqamuzane nge-zin-hlobo za-wo u-cezwan-a olu-ncane olu-nyathelisek-a-yo olu-nga-khandw-a ngo-ku-sebenzis-a imi-shini e-vam-ile yo-ku-nyathelis-a ngo-ku-s-esi-linganis-weni nge-s-enti eli-lodw-a l-a-s-emelik-a  ngo-msombuluko ososayensi ba-s-estanford u-ni-versity-a schol of m-edicin-e ba-memezel-e ithuluzi eli-sh-a elikw-azi uku-hlukanis-a amagqamuzane nge-zinhlobo za-wo u-cezwan-a olu-ncane olu-nyathelisek-a-yo olunga-khandw-a ngo-ku-sebenzis-a imishini evam-ile yo-ku-nyathelis-a ngo-ku-s-esi-linganisweni nge-senti elilodwa la-s-emelika  on monday scientists from the stanford university school of medicine announced the invention of a new diagnostic tool that can sort cells by type a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one us cent each\n",
      "1                               abacwaningi abahola phambili bathi lokhu kungaholela ekushesheni kutholeke umhlavuza isifo sofuba igciwane lesandulela ngculazi kanye nomalaleveva ezigulini ezisemazweni ampofu lapho amazinga okusinda ezifweni ezinjengomdlavuza webele angaba nguhhafuwamazwe acebile\\n                                      a-ba-cwaning-i ab-a-hol-a phambili ba-thi lo-khu ku-nga-holel-a e-ku-shesh-e-ni ku-tholek-e u-m-hlavuz-a i-si-fo so-fuba i-gciwane le-s-andulela ngculazi ka-nye n-o-malaleveva e-zi-gul-ini ezi-s-e-ma-zw-eni a-mpofu la-pho a-ma-zinga oku-sind-a e-zi-f-weni ezi-njengomdlavuz-a w-ebel-e a-ng-ab-a ng-u-hhafuwamazwe a-ceb-ile                                      a-ba-cwaning-i ab-a-hol-a phambili ba-thi lo-khu ku-nga-holel-a e-ku-shesh-e-ni ku-tholek-e um-hlavuza isi-fo so-fuba i-gciwane le-s-andulela ngculazi ka-nye no-malaleveva ezi-gul-ini ezi-s-ema-zw-eni a-m-pofu la-pho ama-zinga o-ku-sind-a ezi-f-weni ezi-njengomdlavuz-a w-ebel-e a-ngab-a ng-u-hafuwamazw-e a-ceb-ile                                          abacwaningi aba-hol-a phambili ba-thi lo-khu ku-nga-holel-a e-ku-shesh-e-ni ku-tholek-e umhlavuza isifo so-fuba igciwane le-sandulela ngculazi k-any-e no-malaleveva ezi-gulini ezis-ema-zweni ampofu la-pho amazinga oku-sind-a ezi-fweni ezinjengomdlavuza w-ebel-e angaba ng-uhafuwamazwe aceb-ile                                     lead researchers say this may bring early detection of cancer tuberculosis hiv and malaria to patients in lowincome countries where the survival rates for illnesses such as breast cancer can be half those of richer countries\n",
      "2                                                                                                                                   ijas c gripen yashayeka emgwaqweni okugijima kuwo indiza cishe ngabo ekuseni ngesikhathi sendawo  utc yaqhuma okwavala izindiza zabantu bonke esikhumulweni sezindiza\\n                                                                                                                                                          i-jas c grip-e-n y-a-shayek-a e-mgwaq-weni oku-gijim-a ku-wo i-ndiza cish-e nga-bo e-ku-s-e-ni ng-e-si-khathi se-ndawo u-tc-e y-a-qhum-a o-kwa-val-a i-zin-diza za-ba-ntu b-onk-e e-si-khumul-weni se-zin-diza                                                                                                                                                        i-jas c grip-en y-a-shayek-a emgwaq-weni o-ku-gijim-a ku-wo in-diza cish-e ng-abo e-ku-s-e-ni nge-si-khathi se-n-dawo u-t y-a-qhum-a o-kw-a-val-a izin-diza za-ba-ntu b-onk-e esi-khumul-weni se-zin-diza                                                                                                                                                 ijas c grip-e-n ya-shayek-a emgwaqweni oku-gijim-a ku-wo indiza cish-e nga-bo e-ku-s-e-ni nge-sikhathi se-ndawo u-tc-a ya-qhum-a okwa-val-a izindiza za-bantu bo-nke esi-khumulweni se-zindiza                                                                                                                                          the jas c gripen crashed onto a runway at around  am local time  utc and exploded closing the airport to commercial flights\n",
      "3                                                                                                                                                                                                                                 umshayeli webhanoyi wahlonzwa njengomholi wesquadron udilokrit pattavee\\n                                                                                                                                                                                                                                                                                          u-m-shayeli we-bhanoyi w-a-hlonzw-a n-jengomholi we-squdadon u-dilokrit patave                                                                                                                                                                                                                                                                                 um-shayeli we-bhanoyi w-a-hlonzw-a njengomholi we-s-qudroron u-dilokriti patav-e                                                                                                                                                                                                                                                                   umshayeli we-bhanoyi wa-hlonzw-a njengomholi we-squadron udilokrit patavav-e                                                                                                                                                                                                        the pilot was identified as squadron leader dilokrit pattavee\n",
      "4                                                                                                                                                                                                 abezindaba bendawo babika ukuthi imoto yomlilo yasesikhumulweni sezindiza yaginqika ngenkathi iphuthuma\\n                                                                                                                                                                                                                                      a-be-zi-n-daba be-ndawo b-a-bik-a uku-thi i-moto yo-m-lilo ya-s-e-si-khumul-weni se-zin-diza y-a-ginqik-a ng-e-n-kathi i-phuthum-a                                                                                                                                                                                                                                    abe-zin-daba be-ndawo b-a-bik-a uku-thi imoto yo-m-lilo ya-s-esi-khumul-weni se-zin-diza y-a-ginqik-a nge-n-kathi i-phuthum-a                                                                                                                                                                                                                          abezindaba be-ndawo ba-bik-a uku-thi imoto yo-mlilo ya-s-esi-khumulweni se-zindiza ya-ginqik-a nge-nkathi i-phuthum-a                                                                                                                                                                                             local media reports an airport fire vehicle rolled over while responding\n",
      "\n",
      "Combined data statistics:\n",
      "Total rows: 2009\n",
      "Total columns: 5\n",
      "Final columns: ['isizulu', 'segmenter_one', 'segmenter_two', 'segmenter_three', 'english']\n",
      "\n",
      "Output saved to: combined_output_20250107_100247.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def combine_csv_and_text(csv_path, text_path, output_path=None, column_names=None, new_column_name='text_content'):\n",
    "    \"\"\"\n",
    "    Combine a CSV file with three columns and a plain text file, with option to rename columns.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file\n",
    "        text_path (str): Path to the text file\n",
    "        output_path (str, optional): Path for output file. If None, generates timestamped name\n",
    "        column_names (list, optional): New names for the CSV columns\n",
    "        new_column_name (str, optional): Name for the column from text file\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved output file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"\\nRead CSV file with shape: {df.shape}\")\n",
    "        print(\"Original CSV columns:\", df.columns.tolist())\n",
    "        \n",
    "        # Rename columns if new names provided\n",
    "        if column_names:\n",
    "            if len(column_names) != len(df.columns):\n",
    "                print(f\"Warning: Number of provided column names ({len(column_names)}) \"\n",
    "                      f\"doesn't match CSV columns ({len(df.columns)})\")\n",
    "                print(\"Will use provided names for available columns\")\n",
    "            \n",
    "            # Rename only the columns we have names for\n",
    "            for i, new_name in enumerate(column_names):\n",
    "                if i < len(df.columns):\n",
    "                    df = df.rename(columns={df.columns[i]: new_name})\n",
    "        \n",
    "        # Read the text file\n",
    "        with open(text_path, 'r', encoding='utf-8') as file:\n",
    "            text_lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "        print(f\"Read {len(text_lines)} lines from text file\")\n",
    "        \n",
    "        # Verify lengths match\n",
    "        if len(text_lines) != len(df):\n",
    "            print(f\"\\nWarning: Length mismatch!\")\n",
    "            print(f\"CSV rows: {len(df)}\")\n",
    "            print(f\"Text lines: {len(text_lines)}\")\n",
    "            shorter_len = min(len(df), len(text_lines))\n",
    "            print(f\"Will use first {shorter_len} entries from each file\")\n",
    "            \n",
    "            # Truncate to shorter length\n",
    "            df = df.iloc[:shorter_len]\n",
    "            text_lines = text_lines[:shorter_len]\n",
    "        \n",
    "        # Add text file content as new column\n",
    "        df[new_column_name] = text_lines\n",
    "        \n",
    "        # Generate output path if not provided\n",
    "        if output_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = f'combined_output_{timestamp}.csv'\n",
    "        \n",
    "        # Save combined data\n",
    "        df.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Print sample of combined data\n",
    "        print(\"\\nFirst few rows of combined data:\")\n",
    "        print(df.head().to_string())\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\nCombined data statistics:\")\n",
    "        print(f\"Total rows: {len(df)}\")\n",
    "        print(f\"Total columns: {len(df.columns)}\")\n",
    "        print(\"Final columns:\", df.columns.tolist())\n",
    "        print(f\"\\nOutput saved to: {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e.filename}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The CSV file is empty\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An unexpected error occurred - {str(e)}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with column renaming\n",
    "    csv_file = \"flores_test_val.csv\"  # CSV with three columns\n",
    "    text_file = \"flores_english_test_val.dev\" # Plain text file\n",
    "    \n",
    "    # Specify new column names\n",
    "    new_column_names = ['isizulu', 'segmenter_one', 'segmenter_two', 'segmenter_three']\n",
    "    \n",
    "    # Combine files with new column names\n",
    "    combined_file = combine_csv_and_text(\n",
    "        csv_file, \n",
    "        text_file,\n",
    "        column_names=new_column_names,\n",
    "        new_column_name='english'  # Name for the text file column\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
