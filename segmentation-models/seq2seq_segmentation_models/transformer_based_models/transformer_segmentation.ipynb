{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dddba3b-ac3a-4b07-a932-4ece23b30715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load from: models_final_200/segmenter_three/model_config.json\n",
      "Config loaded successfully: {'datasource': '../segmentation-models/data/valid_linearizations_4.csv', 'batch_size': 64, 'num_epochs': 60, 'lr': 0.00031050988283798424, 'plateau_patience': 10, 'worsen_patience': 5, 'min_delta': 0.0005, 'verbose': True, 'data_length': 200000, 'seq_len': 50, 'd_model': 128, 'num_layers': 4, 'num_heads': 8, 'd_ff': 512, 'dropout': 0.21404304809404834, 'max_grad_norm': 1.1190287094689317, 'label_smoothing': 0.0008008147200706972, 'vocab_size': 50, 'tokenize_custom': {'tokens': True, 'segmenter_three': True}, 'tokenize_method': 'character', 'file_path': 'models_final_200/segmenter_three', 'model_folder': 'weights', 'lang_src': 'tokens', 'lang_tgt': 'segmenter_three', 'model_basename': 'model_', 'preload': 'latest', 'tokenizer_file': 'tokenizers_{0}.json', 'experiment_name': 'models_final_200/segmenter_three/tensor_data', 'random_seed': 20, 'saved_timestamp': '2025-02-13T00:34:57.316952'}\n",
      "Using device: cuda\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Device memory: 79.253662109375 GB\n",
      "Original shape: (367178, 4)\n",
      "Final shape: (200000, 4)\n",
      "The config file has been saved on models_final_200/segmenter_three\n",
      "Tokenizer path: models_final_200/segmenter_three/tokenizers/tokenizers_tokens.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_three/tokenizers/tokenizers_tokens.json\n",
      "Tokenizer path: models_final_200/segmenter_three/tokenizers/tokenizers_segmenter_three.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_three/tokenizers/tokenizers_segmenter_three.json\n",
      "the dataset length: 200000\n",
      "Max length of source sentence: 30\n",
      "Max length of target sentence: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/pytorch-transformer/seq2seq_train.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 48\n",
      "Model configuration:\n",
      "- d_model: 128\n",
      "- num_layers: 4\n",
      "- num_heads: 8\n",
      "- d_ff: 512\n",
      "- dropout: 0.21404304809404834\n",
      "- label_smoothing: 0.0008008147200706972\n",
      "- max_grad_norm: 1.1190287094689317\n",
      "- lr: 0.00031050988283798424\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from seq2seq_train import Seq2SeqTrainer\n",
    "\n",
    "file_path = os.path.join(\"models_final_200/segmenter_three\", \"model_config.json\")\n",
    "print(f\"Attempting to load from: {file_path}\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"File does not exist!\")\n",
    "else:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_config = json.load(f)\n",
    "            print(\"Config loaded successfully:\", loaded_config)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Reinitialize trainer with new dataset size but keeping optimized parameters\n",
    "trainer = Seq2SeqTrainer(loaded_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a7219e-8297-4e13-876d-119508f76a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ye-zimpawu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.segment_sentence(\"yezimpawu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1892d",
   "metadata": {},
   "source": [
    "### to segment data a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e2729-c953-4328-8393-e3a896cdab2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Segmenter 1\n",
      "Using device: cuda\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Device memory: 79.253662109375 GB\n",
      "Original shape: (367178, 4)\n",
      "Final shape: (200000, 4)\n",
      "The config file has been saved on models_final_200/segmenter_one\n",
      "Tokenizer path: models_final_200/segmenter_one/tokenizers/tokenizers_tokens.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_one/tokenizers/tokenizers_tokens.json\n",
      "Tokenizer path: models_final_200/segmenter_one/tokenizers/tokenizers_segmenter_one.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_one/tokenizers/tokenizers_segmenter_one.json\n",
      "the dataset length: 200000\n",
      "Max length of source sentence: 30\n",
      "Max length of target sentence: 41\n",
      "Loaded best model from epoch 20\n",
      "Model configuration:\n",
      "- d_model: 128\n",
      "- num_layers: 3\n",
      "- num_heads: 16\n",
      "- d_ff: 1024\n",
      "- dropout: 0.18953066758095358\n",
      "- label_smoothing: 0.000949605759638339\n",
      "- max_grad_norm: 0.5246939924778026\n",
      "- lr: 0.0004800819901770108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/pytorch-transformer/seq2seq_train.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10000/500000 with segmenter_one\n",
      "Saving progress at sentence 10000 for segmenter_one...\n",
      "Processing sentence 18990/500000 with segmenter_one"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 62462/500000 with segmenter_one"
     ]
    }
   ],
   "source": [
    "from seq2seq_train import Seq2SeqTrainer\n",
    "import pandas as pd\n",
    "def batch_segment_with_models(sentences_df, config_paths, text_column='zulu', save_interval=2000):\n",
    "    \"\"\"\n",
    "    Segment multiple sentences using multiple models, with periodic saves to distinct files\n",
    "    \"\"\"\n",
    "    results = {f'segmenter_{i}': [] for i in range(1, len(config_paths) + 1)}\n",
    "    skipped_words = {f'segmenter_{i}': [] for i in range(1, len(config_paths) + 1)}\n",
    "    # sentences = sentences_df[text_column].tolist() \n",
    "    sentences = sentences_df\n",
    "    for i, config_path in enumerate(config_paths, 1):\n",
    "        print(f\"\\nProcessing with Segmenter {i}\")\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "    \n",
    "        trainer = Seq2SeqTrainer(config)\n",
    "        \n",
    "        # Initialize model-specific results file\n",
    "        model_filename = f'diversified_data_{config[\"lang_tgt\"]}.csv'\n",
    "        columns = ['original', config[\"lang_tgt\"]]\n",
    "        pd.DataFrame(columns=columns).to_csv(model_filename, index=False)\n",
    "\n",
    "        for j, sentence in enumerate(sentences, 1):\n",
    "            words = sentence.strip().split()\n",
    "            segmented_words = []\n",
    "            current_skipped = []\n",
    "            for word in words:\n",
    "                # Check token length before processing\n",
    "                token_length = len(trainer.tokenizer_src.encode(word).ids) + 2\n",
    "\n",
    "                if token_length > 50:\n",
    "                    print(f\"\\nSkipping word '{word}' (length: {token_length})\")\n",
    "                    current_skipped.append({\n",
    "                        'word': word,\n",
    "                        'length': token_length,\n",
    "                        'sentence_idx': j-1,\n",
    "                        'sentence': sentence\n",
    "                    })\n",
    "                    segmented_words.append(f\"[SKIPPED:{word}]\")\n",
    "                else:\n",
    "                    segmented = trainer.translate(word).strip()\n",
    "                    segmented_words.append(segmented)\n",
    "            \n",
    "            result = \" \".join(segmented_words)\n",
    "            results[f'segmenter_{i}'].append(result)\n",
    "            skipped_words[f'segmenter_{i}'].extend(current_skipped)\n",
    "            \n",
    "            print(f\"Processing sentence {j}/{len(sentences)} with {config['lang_tgt']}\", end='\\r')\n",
    "            \n",
    "            # Save progress periodically\n",
    "            if j % save_interval == 0 or j == len(sentences):\n",
    "                print(f\"\\nSaving progress at sentence {j} for {config['lang_tgt']}...\")\n",
    "                \n",
    "                current_df = pd.DataFrame({\n",
    "                    'original': sentences[:j],\n",
    "                    config[\"lang_tgt\"]: results[f'segmenter_{i}'][:j]  \n",
    "                })\n",
    "                \n",
    "                # Save updated results\n",
    "                current_df.to_csv(model_filename, index=False)\n",
    "                \n",
    "                # Save skipped words\n",
    "                if current_skipped:\n",
    "                    pd.DataFrame(skipped_words[f'segmenter_{i}']).to_csv(\n",
    "                        f'skipped_words_{config[\"lang_tgt\"]}.csv', index=False\n",
    "                    )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    final_results = {'isizulu': sentences}\n",
    "    \n",
    "    for i, config_path in enumerate(config_paths, 1):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        final_results[config[\"lang_tgt\"]] = results[f'segmenter_{i}']\n",
    "    \n",
    "    final_df = pd.DataFrame(final_results)\n",
    "    final_df.to_csv('diversified_data.csv', index=False)\n",
    "    \n",
    "    return final_df, skipped_words\n",
    "\n",
    "\n",
    "with open(\"random_sample_500k.txt\", \"r\") as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "config_paths = [\n",
    "    \"models_final_200/segmenter_one/model_config.json\",\n",
    "    \"models_final_200/segmenter_two/model_config.json\",\n",
    "    \"models_final_200/segmenter_three/model_config.json\"\n",
    "]\n",
    "\n",
    "results_df, skipped_words = batch_segment_with_models(\n",
    "    sentences, \n",
    "    config_paths, \n",
    "    text_column='zulu',\n",
    "    save_interval=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c492d0a2-f2b4-4d85-85de-ddffbd8dd192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Segmenter 1\n",
      "Using device: cuda\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Device memory: 79.253662109375 GB\n",
      "Original shape: (367178, 4)\n",
      "Final shape: (200000, 4)\n",
      "The config file has been saved on models_final_200/segmenter_one\n",
      "Tokenizer path: models_final_200/segmenter_one/tokenizers/tokenizers_tokens.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_one/tokenizers/tokenizers_tokens.json\n",
      "Tokenizer path: models_final_200/segmenter_one/tokenizers/tokenizers_segmenter_one.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_one/tokenizers/tokenizers_segmenter_one.json\n",
      "the dataset length: 200000\n",
      "Max length of source sentence: 30\n",
      "Max length of target sentence: 41\n",
      "Loaded best model from epoch 20\n",
      "Model configuration:\n",
      "- d_model: 128\n",
      "- num_layers: 3\n",
      "- num_heads: 16\n",
      "- d_ff: 1024\n",
      "- dropout: 0.18953066758095358\n",
      "- label_smoothing: 0.000949605759638339\n",
      "- max_grad_norm: 0.5246939924778026\n",
      "- lr: 0.0004800819901770108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/pytorch-transformer/seq2seq_train.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Segmenter 2\n",
      "Using device: cuda\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Device memory: 79.253662109375 GB\n",
      "Original shape: (367178, 4)\n",
      "Final shape: (200000, 4)\n",
      "The config file has been saved on models_final_200/segmenter_two\n",
      "Tokenizer path: models_final_200/segmenter_two/tokenizers/tokenizers_tokens.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_two/tokenizers/tokenizers_tokens.json\n",
      "Tokenizer path: models_final_200/segmenter_two/tokenizers/tokenizers_segmenter_two.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_two/tokenizers/tokenizers_segmenter_two.json\n",
      "the dataset length: 200000\n",
      "Max length of source sentence: 30\n",
      "Max length of target sentence: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/pytorch-transformer/seq2seq_train.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 6\n",
      "Model configuration:\n",
      "- d_model: 256\n",
      "- num_layers: 6\n",
      "- num_heads: 16\n",
      "- d_ff: 1024\n",
      "- dropout: 0.27119126273901606\n",
      "- label_smoothing: 0.014005492657761388\n",
      "- max_grad_norm: 1.1819095309366932\n",
      "- lr: 0.0004197711726342262\n",
      "\n",
      "Processing with Segmenter 3\n",
      "Using device: cuda\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Device memory: 79.253662109375 GB\n",
      "Original shape: (367178, 4)\n",
      "Final shape: (200000, 4)\n",
      "The config file has been saved on models_final_200/segmenter_three\n",
      "Tokenizer path: models_final_200/segmenter_three/tokenizers/tokenizers_tokens.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_three/tokenizers/tokenizers_tokens.json\n",
      "Tokenizer path: models_final_200/segmenter_three/tokenizers/tokenizers_segmenter_three.json\n",
      "Loading existing tokenizer from models_final_200/segmenter_three/tokenizers/tokenizers_segmenter_three.json\n",
      "the dataset length: 200000\n",
      "Max length of source sentence: 30\n",
      "Max length of target sentence: 36\n",
      "Loaded best model from epoch 48\n",
      "Model configuration:\n",
      "- d_model: 128\n",
      "- num_layers: 4\n",
      "- num_heads: 8\n",
      "- d_ff: 512\n",
      "- dropout: 0.21404304809404834\n",
      "- label_smoothing: 0.0008008147200706972\n",
      "- max_grad_norm: 1.1190287094689317\n",
      "- lr: 0.00031050988283798424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/pytorch-transformer/seq2seq_train.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['score'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmenter_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_b_segmented_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmenter_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_b_segmented_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmenter_3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_b_segmented_3\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_word\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Save segmented results\u001b[39;00m\n\u001b[1;32m     57\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmented_words_pairs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['score'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from seq2seq_train import Seq2SeqTrainer\n",
    "\n",
    "def segment_words(words, config_paths):\n",
    "    \"\"\"\n",
    "    Segments a list of words using multiple segmentation models.\n",
    "    Returns a dictionary with segmented outputs.\n",
    "    \"\"\"\n",
    "    segmented_results = {f'segmenter_{i}': [] for i in range(1, len(config_paths) + 1)}\n",
    "\n",
    "    for i, config_path in enumerate(config_paths, 1):\n",
    "        print(f\"\\nProcessing with Segmenter {i}\")\n",
    "\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "    \n",
    "        trainer = Seq2SeqTrainer(config)\n",
    "        \n",
    "        for word in words:\n",
    "            segmented = trainer.translate(word).strip()\n",
    "            segmented_results[f'segmenter_{i}'].append(segmented)\n",
    "\n",
    "    return segmented_results\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"1.csv\", delimiter=\";\")\n",
    "\n",
    "# Get all unique words from word_a and word_b\n",
    "unique_words = list(set(df[\"word_a\"].tolist() + df[\"word_b\"].tolist()))\n",
    "\n",
    "# Define paths to your segmentation models\n",
    "config_paths = [\n",
    "    \"models_final_200/segmenter_one/model_config.json\",\n",
    "    \"models_final_200/segmenter_two/model_config.json\",\n",
    "    \"models_final_200/segmenter_three/model_config.json\"\n",
    "]\n",
    "\n",
    "# Segment all words\n",
    "segmented_words = segment_words(unique_words, config_paths)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "segmented_df = pd.DataFrame(segmented_words)\n",
    "segmented_df[\"original_word\"] = unique_words\n",
    "\n",
    "# Merge with original dataset\n",
    "df = df.merge(segmented_df, left_on=\"word_a\", right_on=\"original_word\", how=\"left\")\n",
    "df.rename(columns={\"segmenter_1\": \"word_a_segmented_1\", \"segmenter_2\": \"word_a_segmented_2\", \"segmenter_3\": \"word_a_segmented_3\"}, inplace=True)\n",
    "df.drop(columns=[\"original_word\"], inplace=True)\n",
    "\n",
    "df = df.merge(segmented_df, left_on=\"word_b\", right_on=\"original_word\", how=\"left\")\n",
    "df.rename(columns={\"segmenter_1\": \"word_b_segmented_1\", \"segmenter_2\": \"word_b_segmented_2\", \"segmenter_3\": \"word_b_segmented_3\"}, inplace=True)\n",
    "df.drop(columns=[\"original_word\"], inplace=True)\n",
    "df.drop(columns='score', inplace=True)\n",
    "\n",
    "# Save segmented results\n",
    "df.to_csv(\"segmented_words_pairs.csv\", index=False)\n",
    "print(\"Segmentation complete. Results saved to segmented_words.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
