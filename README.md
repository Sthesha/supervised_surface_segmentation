# isiZulu Grammar-Based Morphological Segmentation and Translation Pipeline

This project provides a complete pipeline for generating linguistically-informed segmentation data from isiZulu sentences and training various machine learning models (CRF, LSTM, Transformer) for morphological segmentation. The pipeline further extends to translation tasks, where the transformer-based segmentation model is leveraged to build data-driven translation systems.

---

## Part 1: Data Generation Pipeline (Parsing & Linearization)

We use a GF-based grammar to parse isiZulu sentences into abstract syntax trees and linearize them into segmented forms. Two Bash scripts automate the data generation:

### Step 1: Parse isiZulu Sentences (parsing folder)

Run inside the Docker container:

The folder for parsing has multiple items: bash and python scripts, grammars, and corpora used to parse and linearise to generate data.

```bash
bash bash_scripts/parse_json.sh
```
This will:
- Batch and parse isiZulu sentences from `various_zulu_corpora_folder.json`
- Save parsed trees to `parses/various_zulu_corpora_folder/batches/*.json`
- These JSON files are consolidated into one big JSON file and text file.
- The final file here is a CSV file that contains sentences and their two respective abstract syntax trees.

### Step 2: Augment and Linearize Trees
```bash
bash bash_scripts/data_generation.sh
```
This script performs:
1. Subtree extraction from parsed trees, to get useful subtrees that do not contain `MkSymb`, which indicates a word that could not be parsed.
2. These trees undergo augmentation by changing different syntactic and lexical items. See Python script: `linguistic_tree_augmenter.py`
3. Both the augmented trees and the original subtrees undergo linearization using multiple grammars (Lin A, Lin B and Lin C). See: `linearise_with_grammars.py`
4. Validation of outputs: since some grammars may fail to parse a token, each generated linearised token is validated to ensure only those linearised by all three grammars are kept.

### Output Summary:
- `augmented_trees.txt`: Raw GF trees for augmented sentences
- `valid_augmented_and_linearised.csv`: Clean, valid surface forms
- `valid_unaugmented_and_linearised.csv`: Linearized original parses

These outputs serve as training data for morphological segmentation models.

---

## Part 2: Segmentation Model Training (CRF, LSTM, Transformer) (segmentation-models)

### Models Trained:
1. Conditional Random Field (CRF)
2. LSTM-based Segmenter
3. Transformer-based Segmenter

Each model is trained on both:
- Unaugmented data: Derived from the original GF parses
- Augmented data: Created through linguistic transformation and linearization

### Input Data:
- `valid_augmented_and_linearised.csv`
- `valid_unaugmented_and_linearised.csv`

Each CSV contains segmented surface forms at morpheme-level granularity, suitable for supervised training.

The `segmentation-models` folder contains three models, each implemented in Jupyter notebooks that can be used to train supervised surface segmentation models. These notebooks also support evaluation of trained models. In each file, results and outputs generated by the models are saved. Users can evaluate using pre-generated data with the `generate_from_file()` method or run training and evaluation from scratch with custom data. External validation data is also supported.

The parent folder contains a summary of results provided in both `.md` and `.txt` formats.

---

## Part 3: Translation System Training (translation-models)

The Transformer-based segmentation model is integrated into a machine translation pipeline:

1. Segment isiZulu input using the trained Transformer model, as part of the tokenization strategy
2. Perform translation in both directions (isiZulu to English and vice versa)
3. The primary direction investigated is isiZulu to English; the reverse is included for comparison
4. Train translation models using the provided Jupyter notebook: `training_translation_models.ipynb`
5. Evaluate different models using: `evalauate_all_models.ipynb`

This segmentation step significantly improves translation quality by helping the model understand agglutinative morphology.

---

## Docker and System Requirements

Run all scripts inside the `parallel_parsing` Docker container:
```bash
docker exec -it parallel_parsing bash
```

Recommended specifications:
- 64GB+ RAM
- 8+ CPU cores for fast tree linearization
- Python 3.8+
- GF grammars compiled in `grammars/` (PGF)
- PyTorch

---

## Maintainer
**Sthembiso Mkhwanazi**  
NLP Researcher for Low-Resource African Languages  
Email: sithesham at gmail dot com

